{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a823e129-a1ae-42ca-a2ca-2facc381070b",
   "metadata": {},
   "source": [
    "## 1. Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1261d43-b59c-44ed-8edc-4d89a346f3fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "def calculate_metrics(true_labels, predicted_labels,n_classes):\n",
    "  \"\"\"\n",
    "  Calculates F1 score, precision, recall, and per-class AUC for a multiclass classification problem with 3 classes.\n",
    "\n",
    "  Args:\n",
    "    true_labels: A list of true class labels for each sample.\n",
    "    predicted_labels: A list of predicted class labels for each sample.\n",
    "    n_classes: total number of classes (>2)\n",
    "\n",
    "  Returns:\n",
    "    A dictionary containing the F1 score, precision, recall (macro-averaged), and per-class AUC.\n",
    "  \"\"\"\n",
    "\n",
    "  # Check if the lengths of the lists are equal\n",
    "  if len(true_labels) != len(predicted_labels):\n",
    "    raise ValueError(\"Length of true labels and predicted labels must be equal.\")\n",
    "\n",
    "  # Calculate F1 score and macro-averaged precision and recall\n",
    "  f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "  precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "  recall = recall_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "  # Calculate per-class AUC\n",
    "  auc_scores = []\n",
    "  for class_label in range(n_classes):\n",
    "    auc_scores.append(roc_auc_score(true_labels, predicted_labels, multi_class='ovo'))\n",
    "\n",
    "  # Return metrics as a dictionary\n",
    "  return {\n",
    "      \"f1_score\": f1,\n",
    "      \"precision\": precision,\n",
    "      \"recall\": recall,\n",
    "      \"auc_per_class\": auc_scores\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52ad8d1-76a6-4156-9823-c3ef2dfc1ae7",
   "metadata": {},
   "source": [
    "## 2. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/train_data.csv\", index_col=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change datatypes\n",
    "\n",
    "df['a'] = pd.to_datetime(df['a']).astype('int64')\n",
    "df['b'] = pd.to_datetime(df['b']).astype('int64')\n",
    "df['c'] = pd.to_datetime(df['c']).astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7601ce3-82e8-4d87-9a0e-1705f2bf7673",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "Select the optimal features suggested by the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['a',\n",
    "  'b',\n",
    "  'c',\n",
    "  'd'\n",
    "]  \n",
    "\n",
    "# Add the columns to be predicted to the selection\n",
    "select = feature_cols + [\"to_be_predicted\"]\n",
    "\n",
    "# Drop the missing values\n",
    "df.dropna()\n",
    "\n",
    "# Drop the outliers\n",
    "from scipy import stats\n",
    "\n",
    "df = df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]\n",
    "\n",
    "# Partition the dataset to features and predictions\n",
    "X = df[feature_cols]\n",
    "\n",
    "y = df.pop('to_be_predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94869a4b-6cf9-43cd-b590-b19d3ec9fda8",
   "metadata": {},
   "source": [
    "## 4. Train Model\n",
    "Split X and y into training and testing sets and fit the model with data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f31e1c2-9729-4060-9089-2d650d6a50c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecff8cc-2528-43ed-9d8f-dbc8ec776c80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2d7d4d-b410-4c4f-acc9-4f22dd6ea7fb",
   "metadata": {},
   "source": [
    "## 5. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4471de9d-1695-4690-8b06-f6f089f46357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_res = X_test.assign(pnps_class=y_test).assign(pnps_p_class=y_pred)\n",
    "X_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7042b56-9d95-48c2-80ee-47c2a4f1f548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Example usage\n",
    "# You need the labels to binarize\n",
    "labels = ['a', 'b', 'c']\n",
    "true_labels = label_binarize(y_test, classes=labels)\n",
    "predicted_labels = label_binarize(y_pred, classes=labels)\n",
    "\n",
    "metrics = calculate_metrics(true_labels, predicted_labels,n_classes=len(labels))\n",
    "\n",
    "print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
    "print(f\"Macro-averaged Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Macro-averaged Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"AUC per Class: {metrics['auc_per_class']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d6d570-32af-4f7a-b957-bda82d968a2a",
   "metadata": {},
   "source": [
    "## 6. Create the evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fc5659-fc41-402a-b674-c6bc319c2508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "edf = pd.read_csv(\"data/eval_data.csv\", index_col=0)\n",
    "\n",
    "X_eval = edf[feature_cols] # Features\n",
    "y_eval = model.predict(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b3dc1-ebc1-4e2e-8ab7-38906ae9da9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "edf = edf.assign(to_be_predicted=y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662e97b5-97cb-4f15-b94f-59f31b90f59e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "edf[\"to_be_predicted\"].to_csv('results/my_results.csv')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m115"
  },
  "kernelspec": {
   "display_name": "epp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
